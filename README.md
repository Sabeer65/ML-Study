ML-Study: From Mathematical Foundations to Advanced ModelsWelcome to my Machine Learning portfolio. This repository serves as a documented journey of my progress in data science, ranging from low-level mathematical implementations (Gradient Descent from scratch) to high-level predictive modeling using industry-standard libraries.üöÄ Repository Highlights1. Linear Regression (Built from Scratch)Located in the Linear Regression (From scratch) directory, this is a "pure" implementation of the Linear Regression algorithm.Core Logic: Manual implementation of the Gradient Descent optimization algorithm.The Math: Partial derivatives calculated to update slope ($m$) and intercept ($b$) without using sklearn.Key Learnings: Managing the Learning Rate ($L$) to avoid the exploding gradient problem (NaN errors) and optimizing Epochs for model convergence.2. Supervised Learning ProjectsThese projects utilize scikit-learn and other frameworks to solve diverse classification and regression problems:Housing (XGBoost): Predictive modeling for real estate prices using Gradient Boosting.Diabetes (SVM): A classification project using Support Vector Machines to predict medical outcomes.Fake News Detection: An NLP-based classification model to distinguish between factual and misinformation.Sonar (Logistic Regression): Binary classification for underwater object detection (Rocks vs. Mines).Iris Flower Classification: The classic "Hello World" of ML species prediction.üß† Deep Dive: Gradient Descent ImplementationIn the "from scratch" projects, I implemented the following logic to find the line of best fit ($y = mx + b$):Loss Function: Used Mean Squared Error (MSE) to quantify the model's error.Gradient Calculation:$m_{gradient} = \frac{2}{n} \sum -x \cdot (y - (mx + b))$$b_{gradient} = \frac{2}{n} \sum -(y - (mx + b))$Optimization: Updating weights by moving against the gradient:$m = m - (m_{gradient} \cdot L)$$b = b - (b_{gradient} \cdot L)$üõ†Ô∏è Tech StackLanguages: Python 3.xData Handling: Pandas, NumPyVisualization: Matplotlib, SeabornLibraries: Scikit-learn, XGBoost, NLP toolkitsüìà Current Progress & Future Goals[x] Implement Linear Regression from scratch.[x] Solve Binary Classification problems (Sonar, Diabetes).[x] Explore Gradient Boosting (XGBoost).[ ] Implement Neural Networks using only NumPy.[ ] Add Feature Scaling (Normalization) to improve from-scratch stability.üìÇ How to NavigateEach folder is self-contained. To run a project:Ensure the required .csv dataset is present in the folder.Run the .py script or open the Jupyter notebook.Observe the plt.show() outputs to see the model's visual performance.

ML-Study: From Mathematical Foundations to Advanced ModelsWelcome to this Machine Learning portfolio. This repository documents a structured progression through data science, moving from low-level mathematical implementations, such as Gradient Descent from scratch, to high-level predictive modeling using industry-standard libraries.Repository Highlights1. Linear Regression (Built from Scratch)Located in the Linear Regression (From scratch) directory, this is a fundamental implementation of the Linear Regression algorithm.The core logic involves a manual implementation of the Gradient Descent optimization algorithm to determine the line of best fit. Instead of relying on high-level libraries like scikit-learn, partial derivatives are calculated to update the slope ($m$) and intercept ($b$).This project specifically addresses technical challenges such as managing the Learning Rate ($L$) to prevent gradient explosion (NaN errors) and optimizing the number of Epochs to ensure model convergence.2. Supervised Learning ProjectsThese projects utilize scikit-learn and other frameworks to address various classification and regression problems across different domains:Housing (XGBoost): Predictive modeling for real estate valuation using Gradient Boosting.Diabetes (SVM): A classification project utilizing Support Vector Machines to predict medical outcomes based on diagnostic data.Fake News Detection: A Natural Language Processing (NLP) classification model designed to distinguish between factual information and misinformation.Sonar (Logistic Regression): Binary classification for underwater object detection, specifically distinguishing between rocks and mines.Iris Flower Classification: A foundational classification project for species prediction.Technical Deep Dive: Gradient Descent ImplementationIn the "from scratch" projects, the following logic was implemented to minimize the cost function ($y = mx + b$):Loss Function: Mean Squared Error (MSE) was utilized to quantify the variance between predicted and actual values.Gradient Calculation:$m_{gradient} = \frac{2}{n} \sum -x \cdot (y - (mx + b))$$b_{gradient} = \frac{2}{n} \sum -(y - (mx + b))$Optimization: Weights are updated by iterating against the gradient to find the local minimum:$m = m - (m_{gradient} \cdot L)$$b = b - (b_{gradient} \cdot L)$Technology StackProgramming Languages: Python 3.xData Manipulation: Pandas, NumPyData Visualization: Matplotlib, SeabornFrameworks and Libraries: Scikit-learn, XGBoost, NLP toolkitsProject Roadmap and Future Objectives[x] Implement Linear Regression from scratch using Gradient Descent.[x] Execute Binary Classification models (Sonar, Diabetes).[x] Utilize Gradient Boosting frameworks (XGBoost).[ ] Develop Neural Networks using exclusively NumPy.[ ] Implement Feature Scaling (Normalization/Standardization) to improve the stability of custom algorithms.Navigation InstructionsEach directory is self-contained. To execute a project, verify that the required .csv dataset is present within the specific project directory. Execute the .py script or open the provided Jupyter notebook, and review the graphical outputs to evaluate the model's performance and convergence.
